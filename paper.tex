\documentclass[times,sort&compress]{elsarticle}
\usepackage{ycviu}
\usepackage{framed}
\usepackage{multirow}
\usepackage{amssymb}
\usepackage{latexsym}
\usepackage{url}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{subcaption}
\usepackage{amsmath}
\usepackage{listings}
\usepackage{booktabs}
\usepackage{amsthm}
\usepackage{graphicx}

\graphicspath{{images/}}


\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\NewDocumentCommand{\codeword}{v}{\texttt{\textcolor{orange}{#1}}}


\definecolor{newcolor}{rgb}{.8,.349,.1}
\definecolor{codegreen}{rgb}{0.7,0.7,0.7}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{1,1,1}
\lstdefinestyle{mystyle}{ backgroundcolor=\color{backcolour},   
commentstyle=\color{codegreen}, keywordstyle=\color{magenta},
numberstyle=\tiny\color{codegray}, stringstyle=\color{codepurple},
basicstyle=\ttfamily\footnotesize, breakatwhitespace=false,         
breaklines=true,                 
captionpos=b,                    
keepspaces=true,                 
numbersep=5pt,                  
showspaces=false,                
showstringspaces=false, showtabs=false,                  
tabsize=2 }
\lstset{style=mystyle}
        

\journal{Computer Vision and Image Understanding}

\begin{document}

\thispagestyle{empty}
                                                             
\begin{table*}[!th]

\begin{minipage}{.9\textwidth}
\baselineskip12pt
\ifpreprint
  \vspace*{1pc}
\else
  \vspace*{-6pc}
\fi

\noindent {\LARGE\itshape Computer Vision and Image Understanding}
\vskip6pt

\noindent {\Large\bfseries Authorship Confirmation}

\vskip1pc


{\bf Please save a copy of this file, complete and upload as the ``Confirmation of
Authorship'' file.}

\vskip1pc

As corresponding author I, \underline{\hphantom{\hspace*{7cm}}}, hereby confirm on
behalf of all authors that:

\vskip1pc

\begin{enumerate}
\itemsep=3pt
\item This manuscript, or a large part of it, \underline {has not been published,  was
not, and is not being submitted to} any other journal. 

\item If \underline {presented} at or \underline {submitted} to or \underline
{published }at a conference(s), the conference(s) is (are) identified and  substantial
\underline {justification for re-publication} is presented  below. A \underline {copy of
conference paper(s) }is(are) uploaded with the  manuscript.

\item If the manuscript appears as a preprint anywhere on the web, e.g. arXiv,  etc., it
is identified below. The \underline {preprint should include a  statement that the paper
is under consideration at Computer Vision and Image Understanding}.

\item All text and graphics, except for those marked with sources, are \underline
{original works} of the authors, and all necessary permissions for  publication were
secured prior to submission of the manuscript.

\item All authors each made a significant contribution to the research reported  and
have \underline {read} and \underline {approved} the submitted  manuscript. 
\end{enumerate}

Signature\underline{\hphantom{\hspace*{7cm}}} Date\underline{\hphantom{\hspace*{4cm}}}
\vskip1pc

\rule{\textwidth}{2pt}
\vskip1pc

{\bf List any pre-prints:} \vskip5pc


\rule{\textwidth}{2pt}
\vskip1pc

{\bf Relevant Conference publication(s) (submitted, accepted, or published):} \vskip5pc



{\bf Justification for re-publication:}

\end{minipage}
\end{table*}

\clearpage
\thispagestyle{empty}
\ifpreprint
  \vspace*{-1pc}
\fi

\begin{table*}[!t]
\ifpreprint\else\vspace*{-15pc}\fi

\section*{Research Highlights (Required)}

To create your highlights, please type the highlights against each \verb+\item+ command. 

\vskip1pc

\fboxsep=6pt \fbox{
\begin{minipage}{.95\textwidth}
It should be short collection of bullet points that convey the core findings of the
article. It should  include 3 to 5 bullet points (maximum 85 characters, including
spaces, per bullet point.)  
\vskip1pc
\begin{itemize}

\item Altering gradients in the backward pass can significantly alter a CNN's
performance.

\item PowerGrad Transform is proposed that enhances the backward pass gradients.

\item Theoretical analysis of the impact of PowerGrad transformation is conducted.

\item Efficacy of PowerGrad Transform is demonstrated through experiments.

\end{itemize}
\vskip1pc
\end{minipage}
}

\end{table*}

\clearpage


\ifpreprint
  \setcounter{page}{1}
\else
  \setcounter{page}{1}
\fi

\begin{frontmatter}

\title{Altering Backward Pass Gradients improves Convergence}

\author[1]{Bishshoy Das\corref{cor1}}
\ead{bishshoy.das@ee.iitd.ac.in}

\author[1]{Milton Mondal\corref{cor2}}
\ead{milton.mondal@ee.iitd.ac.in}

\author[1]{\\ Brejesh Lall}
\ead{brejesh@ee.iitd.ac.in}

\author[1]{Shiv Dutt Joshi}
\ead{sdjoshi@ee.iitd.ac.in}

\author[1]{Sumantra Dutta Roy}
\ead{sumantra@ee.iitd.ac.in}

\cortext[cor1]{Corresponding author}
\cortext[cor2]{Equal contribution}

\address[1]{Electrical Engineering Department, Indian Institute of Technology Delhi, Hauz Khas, New Delhi - 110016, India}

\received{1 May 2013}
\finalform{10 May 2013}
\accepted{13 May 2013}
\availableonline{15 May 2013}
\communicated{S. Sarkar}


\begin{abstract}
In standard neural network training, the gradients in the backward pass are determined
by the forward pass. As a result, the two stages are coupled. This is how most neural
networks are trained currently. However, gradient modification in the backward pass has
seldom been studied in the literature. In this paper we explore decoupled training,
where we alter the gradients in the backward pass. We propose a simple yet powerful
method called PowerGrad Transform, that alters the gradients before the weight update in
the backward pass and significantly enhances the predictive performance of the neural
network. PowerGrad Transform trains the network to arrive at a better optima at
convergence. It is computationally extremely efficient, virtually adding no additional
cost to either memory or compute, but results in improved final accuracies on both the
training and test sets. PowerGrad Transform is easy to integrate into existing training
routines, requiring just a few lines of code. PowerGrad Transform makes it possible for
the network to better fit the training data. With decoupled training, PowerGrad
Transform improves baseline accuracies for ResNet-50 by 0.73\%, for SE-ResNet-50 by
0.66\% and by more than 1.0\% for the non-normalized ResNet-18 network on the ImageNet
classification task.
\end{abstract}

\begin{keyword}
\KWD backpropagation \sep gradients \sep neural networks \sep softmax \sep clipping
\end{keyword}

\end{frontmatter}





\section{Introduction}
\label{sec:Intr}





Backpropagation is traditionally used to train deep neural networks
\cite{lillicrap2020backpropagation}. Gradients are computed using basic calculus
principles to adjust the weights during backpropagation \cite{lecun1988theoretical}.
Alternatives to traditional gradients has rarely been studied in the literature
hitherto. In normal training procedures, gradients are computed immediately in the
backward pass utilizing the values obtained in the forward pass. This makes the backward
pass coupled with the forward pass. However, decoupling the backward pass from the
forward pass by modifying the gradients to improve training efficiency and final
convergent accuracy has hardly been explored. In this paper we explore the landscape of
decoupling the forward pass from the backward pass by altering the gradients and
subsequently updating the network's parameters with the modified gradients. There are
several ways to achieve gradient modification in the backward pass. We discuss a few
techniques in Fig. \ref{fig:gradient_altering}. One could modify the gradients at either
\textbf{(I)} multiple points in the backward computation graph or \textbf{(II)} at a
point very early in the backward graph and allow this changed gradient to also affect
the rest of the network's gradients as it backpropagates through the backward
computation graph.



\begin{figure}[t]
\centering
\includegraphics[width=1.0\columnwidth]{PGT_Types}
\caption{ Different ways of altering gradients in the backward pass. Blue blocks denote
a CNN's different layers (convolutional, linear, batchnorm, softmax, etc.). Orange
blocks indicate the backward graph with unmodified gradients. Green blocks represent
transformation functions, while red blocks indicate transformed gradients. (a)
\textbf{Type 0:} Backpropagation without any gradient modification. (b) \textbf{Type I:}
Altering gradients after they are all computed. (c) \textbf{Type II:} Altering gradients
at an earlier stage in the backward computation graph, subsequently leading to
alteration of all other gradients. }
\label{fig:gradient_altering}
\end{figure}



\textbf{Type 0:} No modification: In this method, we calculate the gradients using the
standard calculus rules and use the chain rule to calculate the gradients of the rest of
the network's parameters, also known as backpropagation (Fig.
\ref{fig:gradient_altering}a). The network is then updated with the gradient descent
equation:

\begin{equation}
\hspace{4cm}
W_i^{t+1} = W_i^t - \lambda\ \nabla_{W_i}(L)
\hspace{2cm}
i=D,D-1,\ldots,1
\end{equation}

\textbf{Type I:} Gradient modification at multiple points independently: In this method,
the gradients are first computed using standard procedure and then individually altered
(Fig. \ref{fig:gradient_altering}b). Gradient clipping \cite{pascanu2013difficulty} and
Adaptive gradient clipping \cite{brock2021high} are examples of such modifications. In
both of these methods, the gradients are first computed using standard rules and then
they are modified using some function. It can be described as:

\begin{equation}
\hspace{4cm}
W_i^{t+1} = W_i^t - \lambda\ f(\nabla_{W_i}(L))
\hspace{2cm}
i=D,D-1,\ldots,1
\end{equation}

where the gradients $\nabla_{W_i}(L)$ are transformed using the transformation function
`$f$' before the weight update.

\textbf{Type II:} Gradient modification at a point very early in the backward graph: In
this type of modification, the gradient is altered at a very early stage in the backward
computation graph and then all subsequent gradients are generated using the values
obtained with the modified gradients (Fig. \ref{fig:gradient_altering}c). Because of the
chain rule, network parameters whose gradients are connected to the point of alteration
in the computation graph also gets subsequently altered. It can be described as:

\begin{align}
\hspace{4cm}
W_D^{t+1} &= W_D^t - \lambda\ f(\nabla_{W_D}(L)) & \\
W_i^{t+1} &= W_i^t - \lambda\ \nabla_{W_i}(L)^{*} &\hspace{-3cm} i=D-1,\ldots,1
\end{align}

where first the gradient $\nabla_{W_D}(L)$ is transformed using the transformation
function `$f$' and then this transformed gradient is propagated through the rest of the
backward graph. All other gradient vectors $\nabla_{W_i}(L)^{*}$ are computed as is, but
because of the early injection of the transformed gradient $\nabla_{W_D}(L)$, all other
gradient vectors that are connected to the transformed gradient through the chain rule
($\nabla_{W_i}(L)^{*}$,\ \ i=D-1,\ldots,1), gets subsequently altered.

Type I modification is computationally more expensive than Type II modification as it
requires altering the gradients of each and every parameter individually. Type II
modification recomputes gradients at each and every location through the natural flow of
backpropagation. We propose PowerGrad Transform (PGT) which is a type II gradient
modification method. With virtually no additional compute or memory requirement,
PowerGrad Transform enhances classification performance of a convolutional neural
network, by leading the network to arrive at a higher quality optima at convergence, at
which both training and test accuracies improve.


PowerGrad Transform modifies the gradients at the softmax layer, which is the earliest
part of most convolutional neural network's backward computation graph. By changing the
gradient at this stage, all other gradients for all other parameters in the network such
as the linear layer, batchnorm statistics and convolutional layer's filter parameters
are affected. We analyze and explore the behaviour of such alteration both
mathematically and through empirical experiments conducted on networks without
batch-normalization. Experimentally, we see that PGT improved training and test
accuracies for both normalized and non-normalized networks. PGT improves a network's
learning capacity and results in a better fit of the training data.



The following are the major contributions of this paper:

\begin{enumerate}

\item We introduce PowerGrad Transform, which decouples the backward and the forward
passes of neural network training and enables a considerably better fit to the dataset,
as assessed by both training and test accuracy measures. PGT is a performance
enhancement method that alters the gradients in the backward pass before the update step
leading to significant boost in network's predictive performance.

\item We perform theoretical analysis of the properties of the PowerGrad transformation
(section \ref{sec:pgt_prop}) and explore its effect on weight parameters and gradients
(section \ref{sec:pgt_weight_gradients}), logits (section
\ref{sec:pgt_logit_derivative}), (section \ref{sec:Effe}), loss values (section
\ref{sec:Effe}) and class separation (section \ref{sec:pgt_class_separation}).

\item We study experimentally the degenerate behaviour of non-BN networks that is often
seen during the normal training procedure, particularly at larger batch sizes. With PGT,
we see improved gradient behaviour and a decreased likelihood of the weights attaining
degenerate states.

\item We provide complete results from a variety of models (non-BN and BN ResNets,
SE-ResNets) using the ImageNet dataset. It helps the network to improve its learning
capabilities by locating a more optimum convergence point. Additionally, we conduct an
ablation study and compare its impacts to those of regularization approaches such as
label smoothing.

\end{enumerate}






\section{Related Works}
\label{sec:Rela}




We examine techniques that are related to gradient modification and emphasize the
distinction between them and our proposed method.




Gradient Clipping (GC): Gradient Clipping \cite{pascanu2013difficulty}, often used in
natural language processing methods \cite{merity2017regularizing}, is a technique that
involves changing or clipping the gradients with respect to a predefined threshold value
during backward propagation through the network and updating the weights using the
clipped gradients \cite{zhang2019gradient, smith2020generalization}. By rescaling the
gradients, the weight updates are likewise rescaled, significantly reducing the risk of
an overflow or underflow \cite{pascanu2012understanding}. GC can be used for training
networks without batch-normalization.

The formulation is as follows: If $G$ is a gradient vector which is $\partial
L / \partial \theta$ (the gradient of the loss $L$ with respect to
the parameters $\theta$). In clip by value, the gradients are modified as:



\begin{equation}
G \rightarrow
\begin{cases}
\lambda \frac{G}{\norm{G}} & \text{if } \norm{G} > \lambda, \\
G & \text{otherwise}
\end{cases}
\end{equation}



At larger batch sizes, the clipping threshold in GC becomes highly sensitive and
requires extensive finetuning for various models, batch sizes, and learning rates. As we
demonstrate later in our studies, GC is not as effective in improving the performance of
non-normalized networks. AGC performs better than GC in non-normalized networks.
However, we show that PGT outperforms both in such networks.




Adaptive Gradient Clipping (AGC): Adaptive Gradient Clipping \cite{brock2021high} is
developed to further enhance backward pass gradients than what is performed by GC. It
takes into account the fact that the ratio of the gradient norm to the weight norm can
provide an indication of the expected change in a single step of optimization. Here the
normalized gradient is clipped by $\lambda$, which means that large weights can have
large gradients and still the normalized gradient can be within $\lambda$. If it is not
within the threshold, then their gradient is normalized by the ratio of norm of the
gradient and norm of the weight in order to avoid gradient explosion. If $W^l$ is the
weight matrix of the $l^\textit{th}$ layer, and $G^l$ is its gradient in the backward
pass, then the following equation is used to modify the gradient of the $i^\textit{th}$
filter before its update:

\begin{equation}
G_{i}^{l}\rightarrow
\begin{cases}
\lambda \frac{\norm{W_{i}^{l} }_{F}^{*}}{\norm{G_{i}^{l} }_{F}} G_{i}^{l} & if\ \frac{\norm{G_{i}^{l} }_{F}}{\norm{W_{i}^{l} }_{F}^{*}} \  >\ \lambda \\
G_{i}^{l} & otherwise
\end{cases}
\end{equation}



The hyperparameter $\lambda$ of AGC is less sensitive to changes in batch size and depth
as observed in \cite{brock2021high}. However, when applied to vanilla residual networks
(ResNet-18), the performance is still afar from normalized variants as demonstrated by
our experiments. In most cases, AGC is used for training networks without
batch-normalization. Our experiments demonstrate that PGT outperforms AGC, when used
independently. When used together, it can further improve a network's performance
(section \ref{sec:Empi}).








Label Smoothing: Label smoothing, introduced by Szegedy et al.
\cite{szegedy2016rethinking}, utilizes smoothing of the ground truth labels as a method
to impose regularization on the logits and the weights of the neural network.



The formulation is as follows. If $q_i$ is the value at the $i^{th}$ index of the one
hot encoded vector of the ground truth label, then the transformed distribution of the
labels $q_i'$ is: \begin{equation} q_i' = \begin{cases} 1-\epsilon & \text{if } i = y,
\\ \frac{\epsilon}{K-1} & otherwise \end{cases} \end{equation} where $\epsilon$ is a
hyperparameter with a value in $[0,1]$ and $y$ denotes the index of the correct class.



Müller et al. investigates how label smoothing work in \cite{DBLP:conf/nips/MullerKH19}.
Using visualizations of penultimate layer activations, they demonstrate that label
smoothing calibrates the networks predictions and aligns them with the accuracies of the
predictions. Parallel works in label smoothing include Pairwise Confusion
\cite{dubey2018pairwise}, combatting Label Noise \cite{reed2014training}, achieving
regularization through intentional label modification \cite{xie2016disturblabel}.

PowerGrad Transform works in a much different way. PGT does not smooth the ground truth
labels, rather it modifies the gradient by smoothing the predicted probability vector.
As we show in section \ref{sec:Math}, our proposed transformation has the opposite
effect of label smoothing. From our studies, we show that PGT in fact leads to larger
generalization gap between the training and test dataset, although it does lead to an
increase in both training and test accuracies at convergence. Coupled with PGT, label
smoothing can narrow the generalization gap, while retaining the benefits of PGT.



Knowledge Distillation: The formulation of PowerGrad Transform is akin to temperature
based logit suppression of \cite{hinton2015distilling}. Knowledge distillation
\cite{DBLP:conf/nips/BaC14} is a process in which two networks are trained. First a
teacher network is trained on a given dataset and then the soft-labels (the predicted
probabilities) of the teacher network are used to train another network, called the
student network. In similar style of label smoothing, knowledge distillation aims to
generate smooth gradient behaviour by forcing the network not to overpredict on any
single image. As in case of label smoothing, the student network's weights are
automatically penalized if the network assigns a probability value higher than the soft
labels generated by the teacher network. Variants of knowledge distillation include
self-distillation \cite{zhang2019your, wang2021memory}, identical student network
distillation \cite{furlanello2018born}, channel distillation \cite{ge2019distilling},
regularizing wrong predictions \cite{yun2020regularizing} and representation or
embedding based knowledge distillation \cite{aguilar2020knowledge, yao2018deep,
passalis2018unsupervised}. Distillation is applied to various tasks such as model
compression \cite{wang2020real}, face recognition \cite{ge2018low}, enhancing ensemble
performance \cite{zhang2018deep}, interpreting neural networks \cite{liu2018improving}.
Efficacy of distillation is extensivly studied in \cite{cho2019efficacy,
yuan2020revisiting}. Applications of logit suppression include: deep metric learning
\cite{zhai2018classification}, non-parametric neighborhood component analysis
\cite{wu2018improving}, sequence-to-sequence modelling \cite{chorowski2016towards},
reinforcement learning \cite{he2018determining} and face recognition
\cite{liu2017sphereface, wang2018cosface}.

The key difference between PGT and distillation methods is that in the latter, the
transformation is applied in the forward pass, while PGT is a backward pass modification
only. Also, in distillation settings the temperature parameter is a part of the
network's computation graph. In the case of PowerGrad Transform, we directly tamper the
gradients without introducing any change in the forward pass. It is a more generalized
way of arriving at altered gradients in the backward pass. Moreover we show that
modifying the gradients in the backward pass can improve performance in many cases. The
rise in training accuracy indicates that PGT enables the network to learn more from the
training data.

Self-knowledge distillation is used in many papers in different ways, which mainly
focuses on improving the performance of the model by either creating multiple ensemble
student models \cite{zhang2018deep}, using different distorted versions of same data
with two different versions of a single network which guides one another
\cite{xu2019data} or injecting multiple sub-modules in the network to improve its
capacity \cite{zhang2019your}. However none of these methods use normal traditional
training without introducing sub-modules in the architecture. PGT differs from
self-knowledge distillation as it neither introduces any additional sub-modules nor
creates different ensembles to improve the performance of the model. PGT follows the
standard neural network training mechanism with a single change which is altering the
last layer's gradients in the backward pass.


Other techniques of weight update include slot machines \cite{aladago2021slot}, where
random weights are generated and carefully selected during backpropagation in a way that
minimizes the loss function.









\section{PowerGrad Transform}
\label{sec:Powe}


\begin{figure}[t]
\centering
\begin{subfigure}{.5\columnwidth}
\centering
\includegraphics[width=0.96\columnwidth]{prob_transform_unordered}
\caption{Unordered distribution}
\end{subfigure}%
\begin{subfigure}{.5\columnwidth}
\centering
\includegraphics[width=0.96\columnwidth]{prob_transform_ordered}
\caption{Rank ordered distribution}
\end{subfigure}
\caption{ Illustration of the impact of PowerGrad Transform on probability values of an
arbitrary distribution (number of classes, $C=10$). $\alpha=1$ indicates the unmodified
distribution. As $\alpha$ reduces from $1$ to $0$, the probability distributions (left:
unordered or right: ordered) becomes flatter and flatter, approaching the uniform
distribution of $1/C$ for every index. }
\label{fig:prob_plots}
\end{figure}



The PowerGrad Transform technique is described in this section. It is a general
technique and can be applied to any neural network. A neural network with parameters $W$
generates $C$ logits denoted by $z$ for every input vector $x$. $z$ is given as $z=Wx$.
Then a set of probability values $p_i$ are generated from the logits using a softmax
function which is defined as $p_i=\frac{e^{z_i}}{\sum _{j=1}^C e^{z_j}}$. $p_i$ and
$z_i$ represent the predicted probability values and the logits for the $i^{th}$ class
respectively. Following this step, the loss function is invoked and the loss between the
predicted probability values the ground truth labels (which is also a probability
distribution) is calculated. If the loss function is cross-entropy loss, then the value
of the loss is given as $L=-\sum _{i=1}^C q_i \log \left(p_i\right)$ where
$q_i$ is the ground truth label of the $i^{th}$ class for a particular training example.
By standard gradient update rule, we can calculate the gradient of the loss with respect
to the logits which takes the form $\frac{\partial L}{\partial z_i}=p_i-q_i$.

The PowerGrad Transform technique is now described. We introduce a hyperparameter
$\alpha$, which takes a value between $[0, 1]$ and regulates the degree of gradient
modification. Fig. \ref{fig:prob_plots} shows the effects of the transform under
different values of $\alpha$ for different distributions. The PowerGrad Transform method
modifies the predicted probability values in the backward pass as follows:

\begin{equation} \hspace*{3.5cm} p_i'=\frac{p_i^{\alpha }}{\sum _{j=1}^C
p_j^\alpha} \ \ \ \ \ \ \ \ i=1,\dots,C\ \ \ \ \ 0 \leq \alpha\leq 1
\label{transformed_probabilities} \end{equation}

The above transformation changes the gradient of the loss with respect to the logits as
follows:

\begin{equation} \widehat{\frac{\partial L}{\partial z_i}}=p_i'-q_i
\label{PGT_logit_derivative}
\end{equation}

The rest of the backward pass proceeds as usual. We denote the original probability
distribution as $P$ (with values $p_i$ at the $i^{th}$ index) and the transformed
distribution as $P'$ (with values $p_i'$ at the $i^{th}$ index).



A code snippet of the algorithm implemented using PyTorch \cite{NEURIPS2019_9015} is
given in the appendix. In PyTorch, the available method to modify gradients in the
backward pass is using the \codeword{register_hook} function which is called in the
\codeword{forward} function. We explore the effect of this change from a theoretical
standpoint in section \ref{sec:Math}. In section \ref{sec:Expe}, we offer empirical data
and experiments to support the proposed method.





\section{Analysis of the PowerGrad transformation}
\label{sec:Math}

In this section, we first describe the properties of the  PowerGrad Transform (PGT) and
then we highlight how these suitable properties of PGT helps the neural network to
improve the performance. We use the same setup as described in section \ref{sec:Powe}.
To explore the properties of PGT, we start by investigating the effect of the transform
on the softmax probabilities.

\subsection{Properties of PGT}
\label{sec:pgt_prop}

\textbf{Lemma 1.} For any arbitrary probability distribution $P$ with probability values
given by $p_i$ for $i=1,\dots,C$, the corresponding transformed probability values
$p_i'$ given by [Eq. \ref{transformed_probabilities}] has a threshold $\Big(\sum
_{j=1}^C p_j^{\alpha}\Big)^{\frac{1}{\alpha-1}}$ and

\begin{equation} \begin{split} p_i' \geq p_i & \text{,\ \ if } p_i \leq
\Big(\sum _{j=1}^C p_j^{\alpha }\Big)^{\frac{1}{\alpha-1}} \\ \hspace*{1cm}
p_i' < p_i & \text{,\ \ if } p_i > \Big(\sum _{j=1}^C
p_j^{\alpha}\Big)^{\frac{1}{\alpha-1}} \end{split} \label{eqn:threshold}
\end{equation}

\textit{Proof.} The proof follows directly from the definition of $p_i'$. We notice that
if $p_i' < p_i$, we get:

\begin{proof}[\unskip\nopunct] \begin{alignat}{2} &\ \ \frac{p_i^{\alpha }}{\sum
_{j=1}^C p_j^{\alpha}} < p_i \\ \Rightarrow &\ p_i^{\alpha -1}<\sum _{j=1}^C p_j^{\alpha
} \\ \Rightarrow &\ p_i > \Big(\sum _{j=1}^C p_j^{\alpha}\Big)^{\frac{1}{\alpha-1}}
\end{alignat} \end{proof}

We call this threshold, the \textit{stationary threshold}. The stationary threshold is
that value of $p_i$ that does not change after the transformation. Therefore, when $p_i$
is greater than the \textit{stationary threshold}, $p_i' < p_i$.



\textbf{Proposition 1.} At $\alpha=0$, the stationary threshold equals $1/C$ and all
values of the transformed distribution $p_i'$ reduces reduces to the uniform
distribution for $i=1,\dots,C$,.

\textit{Proof.} From Eq. (\ref{eqn:threshold}), we see that the stationary threshold at
$\alpha=0$ is $1/C$. Also, following from the definition of the transformed
probabilities (Eq. \ref{transformed_probabilities}) we conclude that if $\alpha=0$, then
all values of $p_i'$ are $1/C$. Therefore the transformed distribution at $\alpha=0$ is
a uniform distribution.

Since we have established that values of $p_i$ which are greater than the stationary
threshold decreases and move down towards the stationary threshold, and values in $p_i$
lower than the stationary threshold moves up towards the stationary threshold, this
transformation makes the distribution more uniform (i.e. it smooths out the actual
distribution) as $\alpha$ is decreased from $1$ and down towards $0$. This final
observation we prove in the following theorem.



\textbf{Theorem 1.} For any arbitrary probability distribution $P$ with probability
values $p_i$ for $i=1,\dots,C$, the stationary threshold of the transformed distribution
$P'$ with probability values $p_i'=\frac{p_i^{\alpha}}{\sum _{j=1}^C p_j^\alpha}, 0 \leq
\alpha \leq 1$ is a monotonically non-decreasing function with respect to $\alpha$.

\textit{Proof.} To prove monotonicity, we first compute the gradient of the stationary
threshold with respect to the variable in concern, $\alpha$.

\begin{alignat}{2} & \frac{\partial }{\partial \alpha} \left(\sum _{j=1}^c
p_j^{\alpha}\right){}^{\frac{1}{\alpha -1}}\ \ = \left(\sum _{j=1}^c
p_j^{\alpha}\right){}^{\frac{1}{\alpha -1}} \left(\frac{\sum _{j=1}^c p_j^{\alpha } \log
\left(p_j\right)}{(\alpha -1) \sum _{j=1}^c p_j^{\alpha}}-\frac{\log \left(\sum _{j=1}^c
p_j^{\alpha}\right)}{(\alpha -1)^2}\right) \\
& = \frac{1}{\alpha  (\alpha -1)^2} \left(\sum _{j=1}^c p_j^{\alpha
}\right){}^{\frac{1}{\alpha -1}} \left(\frac{(\alpha -1) \sum _{j=1}^C p_j^{\alpha }
\log \left(p_j^{\alpha}\right)}{\sum _{j=1}^c p_j^{\alpha }}-\alpha \log \left(\sum
_{j=1}^c p_j^{\alpha }\right)\right) \label{derivative} \end{alignat}

If $a_1,\dots ,a_n$ and $b_1,\dots ,b_n$ are non-negative numbers, then using the log
sum inequality, \\ we get $\sum _{j=1}^n a_j \log \left(\frac{a_j}{b_j}\right)\geq
\left(\sum _{j=1}^n a_j\right) \log \left(\frac{\sum _{j=1}^n a_j}{\sum _{j=1}^n
b_j}\right)$. Setting $a_j=p_j^\alpha$ and $b_j=1$, we get the following lower bound
\begin{equation} \sum _{j=1}^C p_j^{\alpha } \log
\left(p_j^{\alpha }\right)\geq \left(\sum _{j=1}^C
p_j^{\alpha}\right) \log \left(\frac{1}{C} \sum _{j=1}^C
p_j^{\alpha}\right) \label{lower_bound1} \end{equation}

Substituting (\ref{lower_bound1}) in (\ref{derivative}), we get:

\begin{equation} \frac{\partial }{\partial \alpha} \left(\sum
_{j=1}^c p_j^{\alpha}\right){}^{\frac{1}{\alpha -1}}\ \ \geq
\frac{1}{\alpha  (\alpha -1)^2} \left(\sum _{j=1}^c
p_j^{\alpha}\right){}^{\frac{1}{\alpha -1}} \left((1-\alpha )
\log (C)-\log \left(\sum _{j=1}^C p_j^{\alpha }\right)\right)
\label{lb_subs} \end{equation}

$p^{\alpha}$ is concave, and so by Jensen's inequality we get the following upper bound
for the second term:

\begin{equation} \left(\frac{1}{C}{\sum _{j=1}^C
p_j}{}\right){}^{\alpha }\ \ \geq\ \ \frac{1}{C}{\sum _{j=1}^C
p_j^{\alpha }}{} \end{equation}

\begin{equation}\Rightarrow\ \ \log \left(\sum _{j=1}^C p_j^{\alpha}\right)\leq
(1-\alpha ) \log (C) \label{upper_bound} \end{equation}

Substituting (\ref{upper_bound}) in (\ref{lb_subs}),

\begin{proof}[\unskip\nopunct] \begin{equation} \frac{\partial}{\partial \alpha
}\left(\sum _{j=1}^c p_j^{\alpha}\right){}^{\frac{1}{\alpha -1}}\ \ \geq \ \ 0
\end{equation} \end{proof}



We conclude from the analysis that the stationary threshold is a monotonic
non-decreasing function with respect to $\alpha$. Also the derivative of PGT function
with respect to the true probabilities is non-negative which in turn means that the
transformation is an order-preserving map. All values greater than the threshold move
towards the threshold after transformation and all values below the threshold also move
towards the threshold, and the threshold itself moves monotonically towards $1/C$ as
$\alpha$ is decreased from $1$ to $0$. This concludes that the transformation smooths
the original distribution.









\subsection{PGT restricts the partial derivative of the loss w.r.t. each logit from becoming too small or too high}
\label{sec:pgt_logit_derivative}

We know that neural networks uses the softmax function to generate prediction
probabilities from logits in multi-class classification tasks. If we use cross-entropy
loss to train the network then the partial derivative of the loss w.r.t. $i^{th}$ logit
is dependent on the value of the predicted probability and the value of the class label
(either 0 or 1) of the $i^{th}$ logit. 

\begin{equation}
\label{actual_logit_derivative}
\frac{\partial L}{\partial z_i} = p_i -q_i
\end{equation}
In general, the range of the $|\frac{\partial L}{\partial z_i}|$ is [$\delta$,
$\epsilon$] where, $\delta\rightarrow 0$ and $\epsilon\rightarrow 1$ in traditional
training (Eq. \ref{actual_logit_derivative} procedures of neural networks. Here, we
propose a new method of neural network training which alters the gradients at the time
of backward pass by modifying the predicted probability vector using PGT (Eq.
\ref{PGT_logit_derivative}) such that the directional derivative for each logit does not
become too small or too large,


With inclusion of PGT in the last layer, the range of the $\widehat{|\frac{\partial
L}{\partial z_i}|}$ becomes [$\delta^+$, $\epsilon^-$] where, $\delta^+>\delta$ and
$\epsilon^-<\epsilon$


Now, we demonstrate how PGT restricts the magnitude of the directional derivative for
each logit. For a training example, a neural network can assign the highest probability
to either (i) a wrong class or to the (ii) correct class. 

\textbf{(i) Wrong Class Predicted:}
If the network assigns the highest probability for any class except the the true class,
then wrong class assignment happens. We analyze the effect on gradient due to PGT for
this case. There are three possibilities

(a) Let, $i^{th}$ class be the class for which the network assigns low probability, but
it is the true class, then

$p_i < p_{th} \Rightarrow p'_i > p_i \Rightarrow \widehat{\frac{\partial L}{\partial
z_i}} = p'_i -1 >  p_i -1 $ ; where $p_i\rightarrow 0$ \& $q_i = 1$

So; $|\widehat{\frac{\partial L}{\partial z_i}}| = |p'_i -1| <  |p_i -1| \Rightarrow
|\widehat{\frac{\partial L}{\partial z_i}}| <\epsilon $ where; $\epsilon\rightarrow 1$

(b) Let, $j^{th}$ class be the class (not true class) for which the network assigns the
highest probability, then

$p_j > p_{th} \Rightarrow p'_j < p_j \Rightarrow \widehat{\frac{\partial L}{\partial
z_j}} = p'_j <  p_j$ ; where $p_j\rightarrow 1$ \& $q_j = 0$

So, $|\widehat{\frac{\partial L}{\partial z_j}}| <\epsilon$ where; $\epsilon\rightarrow
1$



(c) Let $k$ denote the index for all those classes for which the predicted probability
is low and also the ground truth class label is $0$, then

$p_k < p_{th} \Rightarrow p'_k > p_k \Rightarrow \widehat{\frac{\partial L}{\partial
z_k}} = p'_k >  p_k$ ; where $p_k\rightarrow 0$ \& $q_k = 0$

So, $|\widehat{\frac{\partial L}{\partial z_k}}| >\delta$ where; $\delta\rightarrow$ 0

\textbf{(ii) Correct Class Predicted:}
For a given training example, if the network assigns the highest probability to the true
class then it assigns correct class to the training example. Now we observe the effect
on gradient due to PGT for this case. There are two possibilities,

(a) Let, $i^{th}$ class be the class for which the network assigns the highest
probability and it is also the true class, then

$p_i > p_{th} \Rightarrow p'_i < p_i \Rightarrow \widehat{\frac{\partial L}{\partial
z_i}} = p'_i -1 <  p_i -1 $ ; where $p_i\rightarrow 1$ \& $q_j = 1$

So; $|\widehat{\frac{\partial L}{\partial z_i}}| = |p'_i -1| >  |p_i -1|  \Rightarrow
|\widehat{\frac{\partial L}{\partial z_i}}| >\delta $ where; $\delta\rightarrow 0$

(b) Let, $k^{th}$ index is used for all those classes for which the predicted
probability is low and also the actual label value $0$, then

$p_k < p_{th} \Rightarrow p'_k > p_k \Rightarrow \widehat{\frac{\partial L}{\partial
z_k}} = p'_k >  p_k$ ; where $p_k\rightarrow 0$ \& $q_k = 0$

So, $|\widehat{\frac{\partial L}{\partial z_k}}| >\delta$ where; $\delta\rightarrow 0$

After analyzing all these cases ((i)a,b,c \& (ii)a,b), we are able to deduce that PGT
helps to restrict the directional derivative to be within limit such that for every
logit/direction the directional derivative is not very small or very large. 

\subsection{Effect of PGT on weight gradients}
\label{sec:pgt_weight_gradients}

\begin{figure}[t]
\centering
\includegraphics[scale=1.0,width=0.5\columnwidth]{simplenn.jpg}
\caption{Fully connected layer of a convolutional neural network} 
\label{fig:simplenn}
\end{figure} 




In this section, we discuss the impact of PGT on weight gradients. Let the input and
output of the last convolutional layer of a convolutional neural network be \textbf{x}
and \textbf{a} respectively (Fig. \ref{fig:simplenn}). The last layer which is a fully
connected layer, produces logits \textbf{z} by combining \textbf{a} with weights and
thereafter the network generates the predicted probability vector \textbf{p} from the
logits using softmax activation function. We note that a$_i \geq 0$ $ \forall i$  as,
\textbf{a} is the output of ReLU activation functions.


Here, $z_i = \sum_{j=1}^{C}w_{ij}a_j$, so $\frac{\partial L}{\partial w_{ij}} =
a_j\frac{\partial L}{\partial z_{i}} $. However, if we apply PGT while training the
neural network, then $\widehat{\frac{\partial L}{\partial w_{ij}}}
=a_j\widehat{\frac{\partial L}{\partial z_i}}$

If the activation of the $j^{th}$ neuron is zero, i.e. $a_j = 0$ then there is zero
gradient for all weights which acts on $a_j$ both for normal training and training with
PGT, as $a_j$ has no contribution in the computation of logits. However, the interesting
thing is to observe the effect of PGT when $a_j > 0$. As, the activation values $(a_j)$
are positive, so $ \widehat{\frac{\partial L}{\partial z_i}}  > \frac{\partial
L}{\partial z_{i}} \Rightarrow$ $\widehat{\frac{\partial L}{\partial w_{ij}}} >
\frac{\partial L}{\partial w_{ij}} \forall j \in (1,2,\ldots C)$ and similar
relationship for $a_j < 0$. With PGT, the weight updating rule for $(t+1)^{th}$
iteration then becomes,
\begin{equation}
\label{weight_update_PGT}
w_{ij}^{t+1} = w_{ij}^{t} - \eta \widehat{\frac{\partial L}{\partial w_{ij}^{t}}}
\end{equation}
The weight update equation using PGT, can also be viewed in terms of the following
equation,
\begin{equation}
\label{weight_update_final_PGT}
w_{ij}^{t+1} = w_{ij}^{t} - \frac{\eta}{C_{PGT}} \frac{\partial L}{\partial w_{ij}^{t}}
\end{equation}

For neural networks, the exponential nature of the softmax function produces sharp
distribution for predicted probability vector. For example, if \textbf{z} is $[20, 30,
10]$ then \textbf{p} is equal to $[4\times 10^{-5},  0.99995, 2\times10^{-9}]$. When the
network is at the initial phase of training, most of the training examples are
misclassified. Under this scenario, PGT helps to update the weights in a better manner
than traditional gradients. Suppose the label vector \textbf{q} is $[1, 0, 0]$ then it
indicates that $z_1$ is the logit corresponding to ground truth class and $z_2$ is the
logit where the network has assigned the highest probability. Similarly, $z_3$ is the
logit where the network has assigned a low probability as well as the label vector is
also $0$ for this logit. Here $\frac{\partial L}{\partial w_{1j}^{t}} = \epsilon a_j > 0
\Rightarrow$ $ \widehat{\frac{\partial L}{\partial w_{1j}^{t}}} < \frac{\partial
L}{\partial w_{1j}^{t}} \Rightarrow C_{PGT} > 1$ $\forall j \in (1, 2,\ldots C)$.
Advanced gradient updation algorithms like AdaGrad \cite{duchi2011adaptive} reduces the
gradient in the direction where the directional derivative for a weight is high. It
reduces the learning rate for the weight so that the gradient in that direction is not
too large in order to make the update process less volatile. The formulation of PGT also
implicitly reduces the partial derivative in the same manner for the high derivative
directions, the only difference with the AdaGrad is that $C_{PGT}$ depends only on the
current iteration's gradient and not on previous iterations like AdaGrad. Similarly we
can observe that for this example, $\frac{\partial L}{\partial w_{3j}^{t}} = \delta a_j
\approx 0 \Rightarrow$ $  \widehat{\frac{\partial L}{\partial w_{3j}^{t}}} >
\frac{\partial L}{\partial w_{3j}^{t}} \Rightarrow C_{PGT} < 1$ $\forall j \in (1,
2,\ldots C)$. As directional derivative for the weights associated with third logit is
extremely low, PGT helps to increase the derivative in these directions so that the
training process does not become too slow.



\subsection{Effect of PGT on class separation}
\label{sec:pgt_class_separation}

During final iterations of training, we analyze the iteration-wise gradient behaviour
where the network assigns the highest probability to the correct class for a given
training example; we deduce the effects of PGT in the next immediate training iteration.
For example, if \textbf{z} is $[30, 20, 10]$ then \textbf{p} is $[0.99995, 4\times
10^{-5}, 2\times10^{-9}]$ and \textbf{q} is $[1, 0, 0]$. There is no change in the
weight update due to PGT when $a_j = 0$. However, when $i \neq 1$ and $a_j > 0$,
$|\widehat{\frac{\partial L}{\partial w_{1j}^{t}}}| > |\frac{\partial L}{\partial
w_{1j}^{t}}|$ and $\widehat{\frac{\partial L}{\partial w_{ij}^{t}}} > \frac{\partial
L}{\partial w_{ij}^{t}}$. Here in this example $\frac{\partial L}{\partial z_{1}^{t}} <
0 \Rightarrow \frac{\partial L}{\partial w_{1j}^{t}} < 0 \Rightarrow$
$\widehat{w_{1j}^{t+1}} > w_{1j}^{t+1} > w_{1j}^{t} \forall j$. We also know that
$z_i^{t+1} = \sum_{j=1}^{C}w_{ij}^{t+1}a_j$ and $\widehat{z_i^{t+1}} =
\sum_{j=1}^{C}\widehat{w_{ij}^{t+1}}a_j$. It indicates that $\widehat{z_{1}^{t+1}} >
z_{1}^{t+1} > z_{1}^{t}$ as all the weights associated with first logit is larger while
using PGT in $(t+1)^{th}$ iteration. Similarly, for the non-true classes
$\widehat{w_{ij}^{t+1}} < w_{ij}^{t+1} < w_{ij}^{t}$ when $i \neq 1$ $\forall j$ as
$\frac{\partial L}{\partial z_{i}^{t}} > 0$. So in $(t+1)^{th}$ iteration,
$\widehat{z_{i}^{t+1}} < z_{i}^{t+1} < z_{i}^{t}$ when $i \neq 1$. We observe that
$dist(\widehat{z_{1}^{t+1}}, \widehat{z_{i}^{t+1}}) > $  $dist(z_{1}^{t+1},
z_{i}^{t+1})$ where the first logit corresponds to the true class and all other logits
indexed by $i$ are non-true classes. Therefore the distance between correct and
incorrect class logits increases due to PGT and this leads to better class separation.


\section{Experiments}
\label{sec:Expe}






\subsection{Experiments on Normalized Networks}
\label{sec:bn}



\begin{table}[t]
\centering
\caption{ Results and comparisons for networks trained on ImageNet-1K. All experiments
are performed with a 100 epoch budget. }
\label{tab:imagenet_table}
\vspace{5pt}
\begin{tabular}{cccccc}
\textbf{Model} & \textbf{Scheduler} & \textbf{Method} & \textbf{Train Acc} &
\textbf{Test Acc} & \textbf{Diff} \\
\midrule
ResNet-18 & Step & - & 69.95 & 69.704 & - \\
& Step & PGT & 70.3 & \textbf{69.844} & 0.14 \\
\midrule
ResNet-18 & Cosine & - & 70.38 & 70.208 & - \\
& Cosine & PGT & 70.53 & \textbf{70.298} & 0.09 \\
\midrule
ResNet-50 & Step & - & 78.99 & 75.97 & - \\
& Step & PGT & 79.56 & \textbf{76.494} & 0.524 \\
\midrule
ResNet-50 & Cosine & - & 79.18 & 76.56 & - \\
& Cosine & PGT & 79.68 & \textbf{77.216} & 0.656 \\
\midrule
ResNet-101 & Cosine & - & 82.29 & 77.896 & - \\
& Cosine & PGT & 83.1 & \textbf{78.258} & 0.362 \\
\midrule
SEResNet-18 & Cosine & - & 71.42 & 71.09 & - \\
& Cosine & PGT & 71.6 & \textbf{71.436} & 0.346 \\
\midrule
SEResNet-50 & Cosine & - & 81.5 & 77.218 & - \\
& Cosine & PGT & 82.47 & \textbf{77.952} & 0.734 \\
\end{tabular}
\end{table}






We perform experiments on various BN variants ResNets using the ImageNet-1K dataset
\cite{deng2009imagenet}. We observe a constant $0.09\%$ to $0.14\%$ rise in ResNet-18's
test accuracy. What's more interesting is that we are seeing comparable improvements in
training accuracy. PGT increases the generalization gap, as evident from this
observation. However, it allows the network to better fit the dataset. In section
\ref{sec:Abla}, we show how combining PowerGrad Transform with regularization techniques
such as label smoothing may improve network performance and simultaneously reduce the
generalization gap.

We find significantly larger improvements in experiments using ResNet-50, a $0.52\%$
increase over the step scheduling baseline and a $0.65\%$ increase over the cosine
scheduling baseline. In studies with Squeeze-and-Excitation networks, we use the cosine
scheduler. In these experiments, we find consistent improvements across the board, with
SEResNet-18 \cite{hu2018squeeze} gaining $0.34\%$ and SEResNet-50 \cite{hu2018squeeze}
gaining $0.73\%$. As we saw with ResNet-18, we find continuous improvements in training
performance because the networks are just training better and arriving to better optimas
at convergence. ResNet-101 training provides a modest advantage of $0.36\%$.

All models are trained on four V100 GPUs with a batch size of $1024$. We utilize the
same set of hyperparameters in each experiment, which are as follows: 100 epoch budget,
5 epochs linear warmup phase beginning with a learning rate of $4\times 10^{-4}$ and
ending with a peak learning rate of $0.4$. In our studies, we employ either a step
scheduler (dividing the learning rate by $10$ at the $30^{th}$, $60^{th}$, and $90^{th}$
epochs) or a cosine decay scheduler\footnote{Reproducible code, training recipes for the
experiments, pretrained checkpoints and training logs are provided at: \\
\url{https://github.com/bishshoy/power-grad-transform}. Code is adapted from
\cite{rw2019timm}.}. We observe a consistent improvement in test metrics when utilizing
cosine scheduling across all of our experiments. Finally, towards the conclusion of
training, we impose a 10 epoch cooling phase in which the network is trained with a
constant learning rate schedule set at $4\times 10^{-4}$. Other hyperparameters include
a weight decay value of $5\times 10^{-4}$ and a momentum of $0.9$ for the SGD Nesterov
optimizer. All of our models are trained using mixed floating point precision. The
findings are shown in Table \ref{tab:imagenet_table}. With these findings, we observe
that our proposed altering gradient mechanism PGT improves the model performance further
even for batch normalized networks.












\subsection{Empirical Studies on Residual Networks without Batch Normalization}
\label{sec:Empi}






This section empirically examines some of the issues that occur when networks are
trained without normalization layers. ResNet-18 \cite{he2016deep} is used as the
foundation model for all empirical trials owing to its popularity among deep learning
practitioners and the relative simplicity with which it can be trained on a big dataset
such as ImageNet \cite{russakovsky2015imagenet}. ResNet-18 is composed of $20$
convolutional layers and one fully connected (FC) layer. At the input, it has a single
convolutional layer with filters of size $7\times 7$. Three convolutional layers are
placed in the downsampling skip connections. The remaining convolutional layers have
several $3\times 3$ filters that are grouped together to create a block. Each basic
block is connected by a skip connection. At the end of all convolutional layers, a
global average pooling layer downsamples the features to a fixed-size feature vector and
transfers it to the FC layer, which fits the features using regression and outputs
logits. The logits $z_i$ produced by the FC layer are fed into a softmax layer, which
transforms them to predicted probabilities $p_i$ using the equation $p_i=\frac
{e^{z_i}}{\sum _i e^{z_i}}$.

We concentrate on the following metric: the per-filter L2-norm of each layer. Throughout
the training process, we monitor variations in the per-filter weight norm. Because a
full plot of each layer's data would use an inordinate amount of space, we focus on a
few critical layers' plots. The supplementary section contains plots for each layer.
ResNet-18 without any normalization layers is used for all experiments in this section.
The norm of the weights of the $11\textit{th}$ convolutional layer is shown in Fig.
\ref{fig:norm_plots}a. Layer $11$ includes $256$ filters. Each colour on the figure
represents the evolution of a particular filter throughout the course of training
iterations. The critical point to note is that some filters achieve a norm of zero
during training. This event is referred to as `Zeroing Out', and occurs when a channel
of a weight tensor gets fully filled with zeros. If all the coefficients of the filter
is 0 then only L2 norm of a filter can become zero. If we observe zero filter norm for a
filter, then that filter does not contribute at all to determine the input-output
relationship of a dataset. According to our findings, when a filter tensor becomes
zeroed out, it does not recover with further training.


\begin{figure}[t]
\centering
\captionsetup{font=scriptsize}

\begin{subfigure}[t]{0.32\textwidth}
\includegraphics[width=\linewidth]{baseline-w-layer-4-2}
\caption{Norm of filters of convolutional layer 11 (without PGT)}
\end{subfigure}
\begin{subfigure}[t]{0.32\textwidth}
\includegraphics[width=\linewidth]{baseline-w-layer-7-2}
\caption{Norm of filters of the final convolutional layer (without PGT)}
\end{subfigure}
\begin{subfigure}[t]{0.32\textwidth}
\includegraphics[width=\linewidth]{baseline-f-layer-22-1}
\caption{Norm of final layer's output features (without PGT)}
\end{subfigure}

\begin{subfigure}[t]{0.32\textwidth}
\includegraphics[width=\linewidth]{pgt-w-layer-4-2}
\caption{Norm of filters of convolutional layer 11 (with PGT)}
\end{subfigure}
\begin{subfigure}[t]{0.32\textwidth}
\includegraphics[width=\linewidth]{pgt-w-layer-7-2}
\caption{Norm of filters of the final convolutional layer (with PGT)}
\end{subfigure}
\begin{subfigure}[t]{0.32\textwidth}
\includegraphics[width=\linewidth]{pgt-f-layer-22-1}
\caption{Norm of final layer's output features (with PGT)}
\end{subfigure}
\captionsetup{font=normalsize}
\caption{ Experiment demonstrating the Zeroing Out phenomena, as observed in different
filters and feature vectors. PGT greatly reduces the zeroing out of filters and promote
better learning. }
\label{fig:norm_plots}
\end{figure}





Similar to the plot \ref{fig:norm_plots}a, we see the plot of the final convolutional
layer in Fig. \ref{fig:norm_plots}b, where we observe a number of filters / units
completely dropping out towards the end of training leading to underutilization of the
network's resources. We also plot the final layer's feature norm computed in a per
channel fashion. In Fig. \ref{fig:norm_plots}c, we see the norm of the feature vector at
the output side of the global average pooling layer of ResNet-18. The feature vector
here directly interfaces with the fully connected layer immediately after. Any zeroed
out regions in this tensor directly leads to permanent information loss, as it does not
contribute to the learning of decision boundaries in the upcoming fully connected layer.

We observe both effects \textbf{1)} zeroed-out filter \textbf{2)} zeroed-out feature.
The impact of having a zeroed-out filter or a feature might have a variety of
consequences for training and optimization. If a zeroed out filter is present in a
convolutional layer, there are two possible outcomes. For starters, if the layer is
immediately followed by a fully connected regression layer, there is an abrupt loss of
information. For the reason that a zeroed out filter always generates zeroed out
features regardless of the input image, when this tensor is fed to a fully connected
layer, it does not communicate any information that is useful for classification. In the
event that the zeroed out filter has a skip connection linked across it, this may be
circumvented as long as there are no overlapping zeroed out channels in the two feature
maps.

On the other hand, if a feature tensor has a zeroed out channel at the input of a layer,
the layer creates a zeroed out feature channel at the output, regardless of the weights.
However, if there is a zeroed out channel in the feature map just before the fully
connected layer, then this results in a permanent loss of information as it does not
contribute to the learning of decision boundaries. ResNet-18 suffers from the same kind
of information loss, as the final feature tensor (which is generated after the global
average pooling layer) has multiple channels that are zeroed out [Fig.
\ref{fig:norm_plots}c]. This information collapse phenomenon occurs when a channel that
has been zeroed out stays zeroed out for all images in the dataset.








\begin{figure}[t]
\centering
\includegraphics[width=0.45\columnwidth]{baseline_high_bs-w-layer2-2}
\caption{ Per filter norm vs. iteration plot of layer 5 of an unnormalized ResNet-18,
trained with a very high batch size of 1024. The weight matrix of the entire layer
zeroes out during training. }
\label{fig:high_bs}
\end{figure}





The impact of a feature tensor or weight tensor being zeroed out is seen to be
particularly common at larger batch size training procedures in networks without
normalization layers. As a result, the network's capacity is underutilized since some of
the filters are not involved in extracting any useful information that is important for
performing classification. When working with large batch sizes, it is possible that an
entire layer zeroes out [Fig. \ref{fig:high_bs}]. In a network without skip connections,
this can cause a full collapse in training. For example if a weight tensor completely
zeroes out, and it always produces a zero feature tensor, then the input feature tensor
to the next convolutional layer is a zero tensor and thus the output of the next layer
is always a zero tensor irrespective of its weights, and this continues until the end of
the neural network pipeline leading to zero logits. Also, zeroed out weights tensors
lead to zeroed out gradients hence stopping training for all subsequent iterations
leading to a collapse in training. Although residual networks bypass this difficulty via
the use of the skip connection, it does not solve the layer's non-participation in the
overall objective of classification. It is possible to solve this issue by using methods
such as gradient clipping (GC), adaptive gradient clipping (AGC), and PowerGrad
Transform (PGT), all of which regulate the amount of gradient that flows at various
junctions of the network. GC and AGC modify the gradients at each and every filter of
every layer. PGT performs this operation at only one junction, which is at the softmax
layer.









\begin{figure}[t]
\centering
\captionsetup{font=scriptsize}

\begin{subfigure}[t]{0.32\textwidth}
\includegraphics[width=\linewidth]{baseline-w-layer-5-3}
\caption{Norm of filters of layer 15 (without AGC/PGT)}
\end{subfigure}
\begin{subfigure}[t]{0.32\textwidth}
\includegraphics[width=\linewidth]{pgt-w-layer-5-3}
\caption{Norm of filters of layer 15 (with PGT)}
\end{subfigure}
\begin{subfigure}[t]{0.32\textwidth}
\includegraphics[width=\linewidth]{agc_pgt-w-layer-5-3}
\caption{Norm of filters of layer 15 (with AGC+PGT)}
\end{subfigure}

\begin{subfigure}[t]{0.32\textwidth}
\includegraphics[width=\linewidth]{baseline-w-layer-1-2}
\caption{Norm of filters of layer 2 (without AGC/PGT)}
\end{subfigure}
\begin{subfigure}[t]{0.32\textwidth}
\includegraphics[width=\linewidth]{pgt-w-layer-1-2}
\caption{Norm of filters of layer 2 (with PGT)}
\end{subfigure}
\begin{subfigure}[t]{0.32\textwidth}
\includegraphics[width=\linewidth]{agc_pgt-w-layer-1-2}
\caption{Norm of filters of layer 2 (with AGC+PGT)}
\end{subfigure}
\captionsetup{font=normalsize}
\caption{ Experiment demonstrating the efficacy of PGT and AGC+PGT, as observed in
different layers. }
\label{fig:agc_pgt}
\end{figure}





In a similar fashion to the study in section \ref{sec:Empi}, we examine the norm
profiles of features and weights in the PGT and AGC. The supplementary section includes
all the detailed graphs of each layer. In Fig. \ref{fig:norm_plots}d, we show the
per-filter norm plot of the $11\textit{th}$ convolutional layer as it progresses through
a PGT enabled training session. We observe that the number of zeroed out filters has
considerably reduced. We observe similar benefits in plot of Fig. \ref{fig:norm_plots}e,
which is the plot of the per-filter norm of the final convolutional layer. Coming to
benefits in feature norms, we observe in plot of Fig. \ref{fig:norm_plots}f, the final
feature tensor obtained with PGT enabled training do not contain any zeroed out regions,
leading us to assert that information loss is heavily mitigated as the features pass on
from the feature extracting convolutional layers to the fully connected stage for
regression.

AGC is excellent at eliminating the issue of filter and feature zeroing out. However,
AGC recovers a lower performance benefit than PGT. In the instance of PGT, we see that
it also mitigates the zeroing out phenomenon in most layers. In Fig.\ref{fig:agc_pgt}a,
we see a plot of the $15\textit{th}$ layer of the non-BN ResNet-18 trained without PGT.
As we can see that there has been quite a few filter dropouts in the baseline. PGT
recovers most of the filters [Fig. \ref{fig:agc_pgt}b]. In some layers however, the
performance of PGT worsens. The $2\textit{nd}$ convolutional layer has more zeroed-out
filters in PGT [Fig. \ref{fig:agc_pgt}e] than in the baseline [Fig. \ref{fig:agc_pgt}d].
Despite that, PGT achieves a greater overall accuracy. This necessitates the
simultaneous use of AGC and PGT.

The advantage of combining GC with PGT is not significant. When AGC and PGT are
combined, we see a tremendous increase in accuracy of over $2.06\%$ over the baseline.
As seen in Fig. \ref{fig:agc_pgt}c and \ref{fig:agc_pgt}f, activating AGC and PGT
concurrently results in no filter dropouts in all layers while also improving the
accuracy by a significant margin. Additionally, we give comprehensive graphs of the norm
profiles of the AGC + PGT method for all layers in the supplementary section.


\subsection{Towards removing Normalization}
\label{sec:Towa}



\begin{table}[t]
\centering
\caption{ Results for non-normalized ResNets on ImageNet-1K. Each experiment is
conducted with a 100 epoch budget. }
\label{tab:wobn_table}
\vspace{5pt}
\begin{tabular}{cccccc}
\textbf{Model} & \textbf{Batch Size} & \textbf{Method} & \textbf{Train Acc} &
\textbf{Test Acc} & \textbf{Diff} \\
\midrule
ResNet-18 (non-BN) & 1024 & - & 67.19 & 64.93 & - \\
\midrule
ResNet-18 (non-BN) & 512 & - & 68.02 & 66.552 & - \\
& 512 & PGT & 69.5 & \textbf{67.236} & 0.684 \\
\midrule
ResNet-18 (non-BN) & 256 & - & 68.86 & 66.796 & - \\
& 256 & GC & 69.04 & 67.064 & 0.268 \\
& 256 & AGC & 69.06 & 67.298 & 0.502 \\
& 256 & PGT & 69.97 & \textbf{67.814} & 1.018 \\
& 256 & GC+PGT & 68.67 & 67.088 & 0.292 \\
& 256 & AGC+PGT & 70.92 & \textbf{68.856} & 2.06 \\
\end{tabular}
\end{table}









Experiments are carried out on non-Batch Normalized networks. We specifically utilize
the ResNet-18 model and train it on ImageNet-1K which has 1.28 million images divided
into 1000 classes. ImageNet-1K's test set consists of 50 images per class. Table
\ref{tab:wobn_table} shows the findings.

To begin, we use a batch size of 1024. This is only to illustrate that in unnormalized
networks, accuracy suffers significantly when batch sizes are large. Additionally, the
training process is often unstable, resulting in frequent layer dropouts (as seen in
\ref{sec:Empi}) and sometimes infinite loss values. Thus, for all subsequent tests, we
mostly use batch sizes of 512 and 256. We find $\alpha=0.25$ and $\alpha=0.05$ to be
mostly optimal for ResNet-18 and ResNet-50, though larger values such as $\alpha=0.3$
also have good performance as well. We explore different values of $\alpha$ in section
\ref{sec:Abla}. For ResNet-18, invoking PGT with $\alpha=0.25$ improves the baseline by
$0.68\%$ when the batch size is 512, and by over $1\%$ when the batch size is 256. In
comparison, the improvement obtained by GC and AGC is much less, at $0.27\%$ and
$0.5\%$, respectively.

On the metric of training accuracy, we get a $1.48\%$ increase in training accuracy at a
batch size of 512 and an improvement of $1.11\%$ at a batch size of 256, leading us to
infer that when PowerGrad Transform is used, the network's convergence optima is
significantly superior. We also notice that the training versus testing gap increases
with PGT as compared to the gap obtained at baseline. This allows us to infer that PGT
increases the generalization gap between the training and the test set. The improvement
in training and test accuracies are obtained not through regularization like it does in
methods such as label smoothing. The improvement in both training and test accuracies
are obtained as PGT enables the network to utilize its learning capacity better and
arrive at a better optima at convergence.






\subsection{Effect on Loss, Logits and other Metrics}
\label{sec:Effe}



\begin{figure}[t]
\centering
\begin{subfigure}{.33\columnwidth}
\centering
\includegraphics[width=0.96\columnwidth]{logit_vs_alpha}
\caption{\\Logit vs. $\alpha$}
\end{subfigure}%
\begin{subfigure}{.33\columnwidth}
\centering
\includegraphics[width=0.96\columnwidth]{loss_vs_alpha}
\caption{\\Cross-entropy loss vs. $\alpha$}
\end{subfigure}%
\begin{subfigure}{.33\columnwidth}
\centering
\includegraphics[width=0.96\columnwidth]{logit_vs_loss}
\caption{\\Logit-Loss correlation}
\end{subfigure}
\caption{ Plots of various statistical measures: \textbf{(a)} Variations in the logit
norm vs. $\alpha$. The logit norm is calculated per image over the test set of
ImageNet-1K (at the linear layer of the ResNet-18 architecture) and then averaged.
\textbf{(b)} Variations in the final loss values obtained for different $\alpha$
settings. \textbf{(c)} Correlation between loss and logits. Regression line equation
:$(0.01074 x + 0.25816)$. }
\label{fig:stats}
\end{figure}





As seen in Fig. \ref{fig:stats}a, the logit norm increases as $\alpha$ falls from $1$ to
$0$. In the initial training phase, PGT causes the gradient in both the valid and wrong
classes to rise as the network misclassifies majority of the examples. Due to the
exponential nature of the softmax function, the predicted probability distribution may
frequently be close to a delta distribution, even in instances of misclassifications.
PGT reshapes the predicted probability distribution. It redistributes probability from
the confident classes to the less confident ones. Gradient rises in all classes. We also
see that the final value of the loss rises as $\alpha$ drops, and that the logit norm
and the final value of the loss are linearly correlated Fig. \ref{fig:stats}c. What is
surprising is that greater accuracies may be acquired at larger loss values, at
convergence. This is markedly different from the coupled gradient descent based training
procedures. From these plots of logits and losses we find that with the inclusion of PGT
in the training process, the model can have higher loss values at convergence compared
to the loss when not using PGT. However, this decoupled gradient based training provides
greater accuracies than the standard way of training even though the loss value is
higher.







\begin{figure}[!t]
\centering
\begin{subfigure}{.5\columnwidth}
\centering
\includegraphics[width=0.9\columnwidth]{acc_vs_epoch_r18}
\caption{Plot of training and test accuracies (ResNet-18)}
\end{subfigure}%
\begin{subfigure}{.5\columnwidth}
\centering
\includegraphics[width=0.9\columnwidth]{acc_vs_epoch_r50}
\caption{Plot of training and test accuracies (ResNet-50)}
\end{subfigure}
\caption{ Log-linear plots of training and test accuracies and comparison with baseline
of batch-normalized variants: \textbf{(a)} ResNet-18 ($\alpha=0.25$), \textbf{(b)}
ResNet-50 ($\alpha=0.3$). PGT (PowerGrad Transform) improves upon baseline in all cases.
}
\label{fig:metrics}
\end{figure}









ResNet-18 and ResNet-50 per epoch training and test accuracies with and without PGT are
shown in Fig. \ref{fig:metrics}a,b. In all instances, we observe a gain. This means that
PowerGrad Transform forces the network to learn better representations.


\section{Ablation Study}
\label{sec:Abla}


\begin{table}[t]
\centering
\caption{ Ablation study for ResNet-50 on ImageNet-1K. LS column denotes whether label
smoothing has been applied. A smoothing hyperparameter value of $0.1$ has been used. The
hyperparameter column that controls the amount of PGT gradient modification injected at
softmax, eq. (\ref{transformed_probabilities}). }
\label{tab:ablation_table}
\vspace{5pt}
\begin{tabular}{ccccccc}
\textbf{Model} & \textbf{Scheduler} & \textbf{LS} & \textbf{Hparams} & \textbf{Train
Acc} & \textbf{Test Acc} & \textbf{Gap} \\
\midrule
ResNet-50 & Step &  & - & 78.99 & 75.97 & 3.02 \\
& Step &  & 0.3 & 79.56 & 76.494 & 3.066 \\
& Cosine &  & - & 79.18 & 76.56 & 2.62 \\
& Cosine & $\checkmark$ & - & 78.81 & 76.698 & 2.112 \\
& Cosine &  & 0.3 & 79.43 & 76.886 & 2.544 \\
& Cosine & $\checkmark$ & 0.3 & 78.47 & 76.968 & 1.502 \\
& Cosine &  & 0.05 & 79.68 & \textbf{77.216} & 2.464 \\
& Cosine & $\checkmark$ & 0.05 & 77.69 & 76.39 & 1.3 \\
\end{tabular}
\end{table}




We conduct an ablation study to investigate the consequences of PowerGrad Transform. We
have previously demonstrated from the experiments depicted in section \ref{sec:Expe}
that PowerGrad Transform causes the network to arrive at a better optima and fit a
particular training dataset better than the baseline. We combine our approach with other
regularization techniques such as label smoothing. The findings are shown in the Table
\ref{tab:ablation_table}. We see that adding PowerGrad Transform increases the gap
between the training and test accuracies, while introducing a label smoothing
regularizer decreases the gap. We also investigate the two optimas obtained from
ResNet-50's grid search. PGT with label smoothing improves the accuracy at $\alpha=0.3$
but it hurts the accuracy at $\alpha=0.05$.





\section{Discussions}
\label{sec:Disc}


We introduced PowerGrad Transform, which decouples the backward and forward passes of
neural network training and enables a significantly better fit to the dataset, as
measured by training and test accuracy metrics. We show the application of PowerGrad
Transform, a simple yet powerful technique for modifying the gradient flow behaviour. We
investigate experimentally the degenerate behavior of non-BN networks, which is
frequently observed during the standard training approach, especially with higher batch
sizes. With PGT, gradient behavior is enhanced and the likelihood of weights attaining
degenerate states is diminished. We provide a theoretical analysis of the PowerGrad
transformation. With the use of different network topologies and datasets, we are able
to show the potential of PGT and explore its impacts from an empirical standpoint. We
provide comprehensive results on a number of models (non-BN and BN ResNets, SE-ResNets)
using the ImageNet dataset. PGT helps the network to improve its learning capabilities
by locating a more optimum convergence point. In addition, we undertake an ablation
research and compare its effects to those of regularization methods such label
smoothing.



\bibliographystyle{model2-names}
\bibliography{references}

\newpage

\section{Supplementary section}
\label{sec:Supp}




\subsection{Sample Code}

In PyTorch, the available method to modify gradients in the backward pass is using the
\codeword{register_hook} function which is called in the \codeword{forward} function.

\begin{figure}[h] \hrule \lstinputlisting[language=Python]{code.py} \hrule
\vspace{0.1cm} \caption{Python code of PowerGrad Transform based on PyTorch. }
\label{fig:code}
\end{figure}



\newpage \subsubsection{Norm plots of weights (baseline)} \begin{figure}[h] \centering
\includegraphics[width=0.55\textwidth]{baseline-w_norm} \caption{ Method: baseline. This
is a plot depicting the evolution of the norm of each filter in each layer. Each subplot
corresponds to a layer of the unnormalized ResNet-18. Above each subplot, the layer
indices are shown. Each filter in each subplot is shown with a separate and randomly
determined colour. } \end{figure}

\newpage \subsubsection{Norm plots of weights (baseline (high batch size))}
\begin{figure}[h] \centering
\includegraphics[width=0.55\textwidth]{baseline_high_bs-w_norm} \caption{ Method:
baseline (high batch size). This is a plot depicting the evolution of the norm of each
filter in each layer. Each subplot corresponds to a layer of the unnormalized ResNet-18.
Above each subplot, the layer indices are shown. Each filter in each subplot is shown
with a separate and randomly determined colour. } \end{figure}

\newpage \subsubsection{Norm plots of weights (PGT)} \begin{figure}[h] \centering
\includegraphics[width=0.55\textwidth]{pgt-w_norm} \caption{ Method: PGT. This is a plot
depicting the evolution of the norm of each filter in each layer. Each subplot
corresponds to a layer of the unnormalized ResNet-18. Above each subplot, the layer
indices are shown. Each filter in each subplot is shown with a separate and randomly
determined colour. } \end{figure}

\newpage \subsubsection{Norm plots of weights (AGC + PGT)} \begin{figure}[h] \centering
\includegraphics[width=0.55\textwidth]{agc_pgt-w_norm} \caption{ Method: AGC + PGT. This
is a plot depicting the evolution of the norm of each filter in each layer. Each subplot
corresponds to a layer of the unnormalized ResNet-18. Above each subplot, the layer
indices are shown. Each filter in each subplot is shown with a separate and randomly
determined colour. } \end{figure}



\newpage \subsection{ResNet-18 Layer Index Diagram}

For the layer indices used in figures Fig. \ref{fig:norm_plots}, \ref{fig:high_bs},
\ref{fig:agc_pgt}, we provide a block diagram depicting the different convolutional
layers of ResNet-18 with corresponding layer indices in Fig. \ref{fig:resnet18}. Each
layer represents a convolutional layer, with the layer index and the number of filters
denoted alongside it. Downsampling convolutional layers in skip connections are drawn
and their layer indices are shown on the skip connection paths.

\ \\ \ \\ \ \\ \ \\ \ \\ 

\begin{figure}[h] \centering \includegraphics[width=0.96\columnwidth]{drawing} \caption{
ResNet-18 architecture with layer indices of convolutional layers and the number of
filters in each layer. } \label{fig:resnet18} \end{figure}



\end{document}
